# Human Activity Recognition (HAR) using CNN and Smartphone Accelerometer Data

This project focuses on Human Activity Recognition (HAR) using time-series data collected from a 3-axis accelerometer. The data represents six physical activities ‚Äî walking, walking upstairs, walking downstairs, sitting, standing, and laying ‚Äî performed by volunteers wearing smartphones. We use a 1D Convolutional Neural Network (CNN) to classify these activities based on temporal patterns in the sensor data.

The dataset has been preprocessed into fixed-size windows (128 samples per window) with a sampling frequency of 50Hz. After loading and preprocessing, the model is trained and validated using a typical 85-15 split and evaluated on a held-out test set.

To ensure edge compatibility, the trained Keras model is converted to TensorFlow Lite (TFLite) format, followed by post-training quantization for optimized deployment on low-power devices such as the ESP32 microcontroller. The quantized model is also exported as a C header file for direct integration into embedded firmware.

The project includes full evaluation metrics, such as confusion matrix, accuracy, F1 scores, and inference timing for both TFLite and quantized versions. This pipeline demonstrates an end-to-end workflow from model training to embedded deployment, making it suitable for real-time activity recognition applications on resource-constrained devices.


## üìÅ Dataset

The original dataset is the [UCI HAR Dataset](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones), collected from 30 volunteers using a Samsung Galaxy S II smartphone.

**Note**: Preprocessed CSV versions (`accelerometer_3axis_time_series_train.csv`, etc.) must be placed in the appropriate folder (see `src/train_model.py`).

## Dataset Structure Overview

### Objective
Human Activity Recognition using 3-axis accelerometer data, optimized for TinyML deployment.

### CSV Files Used
| File Name                     | Shape           | Description                  |
|-------------------------------|-----------------|------------------------------|
| `accelerometer_3axis_train.csv` | (7352, 128, 3)  | Training accelerometer data  |
| `accelerometer_3axis_test.csv`  | (2947, 128, 3)  | Test accelerometer data      |
| `y_train_labels.csv`            | (7352, 6)       | One-hot encoded train labels |
| `y_test_labels.csv`             | (2947, 6)       | One-hot encoded test labels  |

### Data Dimensions
- Each sample consists of 128 time steps (2.56 seconds) of accelerometer data over three axes (X, Y, Z).
- Shape per sample: `(128, 3)` representing [X, Y, Z] readings.
- Final splits:
  - Training: `X_train` (6249, 128, 3), `y_train` (6249, 6)
  - Validation: `X_val` (1103, 128, 3), `y_val` (1103, 6)
  - Testing: `X_test` (2947, 128, 3), `y_test` (2947, 6)

### Class Labels (6 Total)
| Label ID | Activity           |
|----------|--------------------|
| 0        | WALKING            |
| 1        | WALKING_UPSTAIRS   |
| 2        | WALKING_DOWNSTAIRS |
| 3        | SITTING            |
| 4        | STANDING           |
| 5        | LAYING             |

### Notes
- The CSV files were generated by processing the original UCI HAR Dataset inertial signal text files.
- The data is normalized and preprocessed for efficient training with 1D Convolutional Neural Networks.
- The training set is further split into 85% training and 15% validation to monitor model performance.

## üß† Model Architecture

A 1D Convolutional Neural Network (CNN) is used:
- Input shape: (128, 3) ‚Äî 3-axis accelerometer data in time windows
- Conv1D ‚Üí MaxPool ‚Üí Conv1D ‚Üí MaxPool ‚Üí Flatten ‚Üí Dense ‚Üí Dropout ‚Üí Output
- Output: Softmax layer for 6-class classification

## üìä Results

| Metric           | Value |
|------------------|-------|
| Validation Acc   | ~X.XX |
| Test Accuracy    | ~X.XX |
| Macro F1-Score   | ~X.XX |
| Micro F1-Score   | ~X.XX |

## üõ†Ô∏è Files & Directories

- `src/`: Python scripts for training, evaluation, model export
- `models/`: Trained Keras, TFLite, and C header model
- `data/`: Dataset instructions or metadata
- `notebooks/`: Optional explorations
- `requirements.txt`: Required Python packages

## üì¶ Dependencies

Install dependencies using:

```bash
pip install -r requirements.txt
