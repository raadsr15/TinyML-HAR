# Human Activity Recognition (HAR) using CNN and Smartphone Accelerometer Data

This project focuses on Human Activity Recognition (HAR) using time-series data collected from a 3-axis accelerometer. The data represents six physical activities — walking, walking upstairs, walking downstairs, sitting, standing, and laying — performed by volunteers wearing smartphones. We use a 1D Convolutional Neural Network (CNN) to classify these activities based on temporal patterns in the sensor data.

The dataset has been preprocessed into fixed-size windows (128 samples per window) with a sampling frequency of 50Hz. After loading and preprocessing, the model is trained and validated using a typical 85-15 split and evaluated on a held-out test set.

To ensure edge compatibility, the trained Keras model is converted to TensorFlow Lite (TFLite) format, followed by post-training quantization for optimized deployment on low-power devices such as the ESP32 microcontroller. The quantized model is also exported as a C header file for direct integration into embedded firmware.

The project includes full evaluation metrics, such as confusion matrix, accuracy, F1 scores, and inference timing for both TFLite and quantized versions. This pipeline demonstrates an end-to-end workflow from model training to embedded deployment, making it suitable for real-time activity recognition applications on resource-constrained devices.


## 📁 Dataset

The original dataset is the [UCI HAR Dataset](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones), collected from 30 volunteers using a Samsung Galaxy S II smartphone.

**Note**: Preprocessed CSV versions (`accelerometer_3axis_time_series_train.csv`, etc.) must be placed in the appropriate folder.

## Dataset Structure Overview

### Objective
Human Activity Recognition using 3-axis accelerometer data, optimized for TinyML deployment.

### CSV Files Used
| File Name                     | Shape           | Description                  |
|-------------------------------|-----------------|------------------------------|
| `accelerometer_3axis_train.csv` | (7352, 128, 3)  | Training accelerometer data  |
| `accelerometer_3axis_test.csv`  | (2947, 128, 3)  | Test accelerometer data      |
| `y_train_labels.csv`            | (7352, 6)       | One-hot encoded train labels |
| `y_test_labels.csv`             | (2947, 6)       | One-hot encoded test labels  |

### Data Dimensions
- Each sample consists of 128 time steps (2.56 seconds) of accelerometer data over three axes (X, Y, Z).
- Shape per sample: `(128, 3)` representing [X, Y, Z] readings.
- Final splits:
  - Training: `X_train` (6249, 128, 3), `y_train` (6249, 6)
  - Validation: `X_val` (1103, 128, 3), `y_val` (1103, 6)
  - Testing: `X_test` (2947, 128, 3), `y_test` (2947, 6)

### Class Labels (6 Total)
| Label ID | Activity           |
|----------|--------------------|
| 0        | WALKING            |
| 1        | WALKING_UPSTAIRS   |
| 2        | WALKING_DOWNSTAIRS |
| 3        | SITTING            |
| 4        | STANDING           |
| 5        | LAYING             |

### Notes
- The CSV files were generated by processing the original UCI HAR Dataset inertial signal text files.
- The data is normalized and preprocessed for efficient training with 1D Convolutional Neural Networks.
- The training set is further split into 85% training and 15% validation to monitor model performance.

## 🧠 Model Architecture

We implement a **1D Convolutional Neural Network (CNN)** optimized for time-series classification. The network captures temporal features from accelerometer sequences and classifies them into six activities.

### 🧩 Architecture Summary
The model processes 3-axis accelerometer data (shape: `128×3`) using 1D convolutional layers followed by pooling, normalization, and dense layers. Here's the full architecture breakdown:

| Layer (Type)                   | Output Shape      | Parameters |
|--------------------------------|-------------------|------------|
| Conv1D (128 filters, kernel=8) | (None, 121, 128)  | 3,328      |
| BatchNormalization             | (None, 121, 128)  | 512        |
| Dropout (rate=0.5)             | (None, 121, 128)  | 0          |
| MaxPooling1D (pool_size=2)     | (None, 60, 128)   | 0          |
| Conv1D (256 filters, kernel=4) | (None, 57, 256)   | 131,328    |
| BatchNormalization             | (None, 57, 256)   | 1,024      |
| Dropout (rate=0.5)             | (None, 57, 256)   | 0          |
| GlobalAveragePooling1D         | (None, 256)       | 0          |
| Dense (64 units, ReLU)         | (None, 64)        | 16,448     |
| Dense (6 units, Softmax)       | (None, 6)         | 390        |
| **Total Parameters**           |                   | **~253,000** |

> ℹ️ The slight difference from 230K is due to additional bias terms and layer configuration.

---

### ✅ Design Highlights

- **1D Convolutions** extract features from sequential sensor inputs.
- **Dropout (0.5)** applied after each conv block to prevent overfitting.
- **GlobalAveragePooling1D** reduces dimensionality and improves generalization.
- **Dense layer** with 64 neurons learns high-level representation.
- **Softmax output** classifies the signal into one of six activities.

---

### 📥 Input Shape

- `128`: Sensor readings collected over 2.56 seconds (50Hz)
- `3`: Accelerometer axes (X, Y, Z)

---

## 🏋️‍♂️ Training Details

The CNN model was trained using supervised learning on the labeled accelerometer time-series data.

### 🔧 Hyperparameters

| Parameter          | Value         |
|--------------------|---------------|
| Optimizer          | Adam          |
| Learning Rate      | 0.001         |
| Loss Function      | Categorical Crossentropy |
| Batch Size         | 64            |
| Epochs             | 30            |
| Validation Split   | 15% (from training set) |
| Early Stopping     | Enabled (patience = 5) |
| Data Augmentation  | None          |
| Input Shape        | (128, 3)      |

### 🧑‍💻 Training Strategy

- Training and validation sets were created from the original training data using an **85-15 split**.
- **Early stopping** was used to avoid overfitting by monitoring validation loss.
- **Dropout and Batch Normalization** were key regularization strategies.
- Model was trained using **Google Colab** with GPU acceleration (optional).

### 📉 Training Observations

![image](https://github.com/user-attachments/assets/59765597-7bf2-4cee-884a-fc8bf8f191d6)


- Training and validation loss curves show fast convergence by epoch 10–12.
- No major overfitting was observed thanks to dropout and validation monitoring.
- Final model selected using best validation performance checkpoint.

## 📊 Model Evaluation Results

The trained model was evaluated on the held-out test set consisting of 2,947 samples. Performance was measured using standard classification metrics.

### ✅ Overall Metrics (on Test Set)

| Metric            | Score    |
|-------------------|----------|
| Accuracy          | **84.53%** |
| F1 Score (Macro)  | **0.8505** |
| Precision (Macro) | **0.8453** |
| Recall (Macro)    | **0.85** |

> 📌 These scores indicate strong generalization across all activity classes, including dynamic and static activities.

---

### 🔍 Confusion Matrix

This matrix shows how well each activity was classified:

![image](https://github.com/user-attachments/assets/db9f75d3-e567-408c-833b-fd7dd0848173)

### 💡 Class-wise Performance Summary


| Activity      | Precision | Recall | F1 Score | Support |
|---------------|-----------|--------|----------|---------|
| **WALK**      | 0.98      | 0.94   | 0.96     | 496     |
| **UPSTAIRS**  | 0.90      | 0.96   | 0.93     | 471     |
| **DOWNSTAIRS**| 0.98      | 0.95   | 0.97     | 420     |
| **SITTING**   | 0.65      | 0.65   | 0.65     | 491     |
| **STANDING**  | 0.72      | 0.75   | 0.74     | 532     |
| **LAYING**    | 0.88      | 0.85   | 0.86     | 537     |

> 📊 Highest performance: **LAYING**, **WALKING_DOWNSTAIRS**  
> 📉 Slight confusion: **SITTING vs STANDING**

## 📦 Model Export & Deployment

To enable real-time activity recognition on low-power edge devices, the trained CNN model is exported to **TensorFlow Lite (TFLite)** format.

### ✔️ TensorFlow Lite Conversion

The Keras model is first converted to standard TFLite format, which significantly reduces the model size and allows inference on embedded systems like the **ESP32**.

### ⚡ Post-Training Quantization

Post-training quantization is applied to further reduce the model footprint and improve latency. This enables faster inference and lower power consumption — ideal for microcontroller-based deployments.

### 🔁 Export as C Header File

The quantized `.tflite` model is converted into a `.h` C header file. This file can be directly integrated into ESP32 firmware using platforms such as **Arduino IDE** or **ESP-IDF**, making it possible to run inference without external model loading.

### ✔️ Deployment Summary

Three model versions were generated for different deployment targets:

| Format                | Size       | Accuracy     | Avg Inference Time (per sample) |
|------------------------|------------|--------------|----------------------------------|
| Keras (.h5)            | 786.56 KB  | 84.53%       | -                                |
| TFLite (Float32)       | 253.66 KB  | 84.53%       | 0.01 ms                          |
| TFLite (Quantized)     | 70.37 KB   | 84.12%       | 0.01 ms                          |

- ✅ **Inference times** were tested in a controlled environment and optimized for single-sample processing.
- ✅ **Quantized model** reduces size by over **90%** compared to the original Keras model with minimal accuracy loss.

---

### ⚠️ Notes & Warnings

- The following warning is expected when using `tf.lite.Interpreter`:
  > `tf.lite.Interpreter is deprecated and scheduled for deletion in TF 2.20.`  
  > **Recommended**: Migrate to **LiteRT Interpreter** via `ai_edge_litert`. [Migration Guide](https://ai.google.dev/edge/litert/migration)

- This does **not affect current inference** and is only relevant for future TensorFlow versions.

---

This export process ensures the model is compact, efficient, and deployable in real-time embedded environments like **ESP32**, **Arduino Nano 33 BLE Sense**, or other TinyML-supported devices.
